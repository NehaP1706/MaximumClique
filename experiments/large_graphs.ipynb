{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fda45a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All algorithms imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# import algos and graphs\n",
    "import sys\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Add parent directory to path so algorithms can be imported\n",
    "sys.path.append(os.path.abspath(\"../algorithms\"))\n",
    "\n",
    "# Import algorithms\n",
    "#from greedy import greedy_clique\n",
    "#from local_search import local_search\n",
    "from randomized import randomized_max_clique\n",
    "from simulated_annealing import simulated_annealing_with_restarts\n",
    "from bron_kerbosch import bron_kerbosch_with_pivot, bron_kerbosch_basic\n",
    "\n",
    "print(\"All algorithms imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Downloading karate_club ...\n",
      "‚ùå Failed to download karate_club: 404 Client Error: Not Found for url: https://github.com/mlabonne/graph-datasets/blob/main/node_classification/karate-club/karate.gml\n",
      "\n",
      "‚¨áÔ∏è Downloading adjnoun_adj ...\n",
      "‚ùå Failed to download adjnoun_adj: 406 Client Error: Not Acceptable for url: https://networkrepository.com/adjnoun-adjacency.php\n",
      "\n",
      "‚ö° Skipping game_of_thrones: already downloaded (raw_graphs/small/)\n",
      "‚¨áÔ∏è Downloading marvel_heroes ...\n",
      "‚úÖ Saved: raw_graphs/small/marvel-hero.csv\n",
      "\n",
      "‚¨áÔ∏è Downloading student_cooperation ...\n",
      "‚úÖ Saved: raw_graphs/small/student-cooperation.graphml\n",
      "\n",
      "‚ö° Skipping flavor_network: already downloaded (raw_graphs/small/)\n",
      "‚¨áÔ∏è Downloading hamsterster ...\n",
      "‚ùå Failed to download hamsterster: 406 Client Error: Not Acceptable for url: https://networkrepository.com/soc-hamsterster.php\n",
      "\n",
      "‚¨áÔ∏è Downloading ogdos_100 ...\n",
      "‚ùå Failed to download ogdos_100: Invalid URL '<link-to-OGDOS-graph-~100nodes>': No scheme supplied. Perhaps you meant https://<link-to-OGDOS-graph-~100nodes>?\n",
      "\n",
      "‚ö° Skipping facebook_combined: already downloaded (raw_graphs/large/facebook_combined.txt.gz)\n",
      "üéØ All requested graph datasets processed.\n"
     ]
    }
   ],
   "source": [
    "#import graphs\n",
    "# --- Download Graph Datasets (Small + Large) ---\n",
    "\n",
    "datasets = {\n",
    "    #\"small\": [\n",
    "        # (\"karate_club\", \"https://github.com/mlabonne/graph-datasets/blob/main/node_classification/karate-club/karate.gml\"),\n",
    "        # (\"adjnoun_adj\", \"https://networkrepository.com/adjnoun-adjacency.php\"),\n",
    "        # (\"game_of_thrones\", \"https://chatox.github.io/networks-science-course/practicum/data/game-of-thrones/\"),\n",
    "        #(\"marvel_heroes\", \"https://chatox.github.io/networks-science-course/practicum/data/marvel-hero.csv\"),\n",
    "        #(\"student_cooperation\", \"https://chatox.github.io/networks-science-course/practicum/data/student-cooperation.graphml\"),\n",
    "        # (\"flavor_network\", \"https://chatox.github.io/networks-science-course/practicum/data/flavor-network/\"),\n",
    "        # (\"hamsterster\", \"https://networkrepository.com/soc-hamsterster.php\"),\n",
    "        # (\"ogdos_100\", \"<link-to-OGDOS-graph-~100nodes>\"),\n",
    "        # (\"brock200_2\", \"https://turing.cs.hbg.psu.edu/txn131/graphs/brock200_2.clq\"),\n",
    "        # (\"c-fat200-5\", \"https://turing.cs.hbg.psu.edu/txn131/graphs/c-fat200-5.clq\"),\n",
    "    #],\n",
    "    \"large\": [\n",
    "        (\"facebook_combined\", \"https://snap.stanford.edu/data/facebook_combined.txt.gz\"),\n",
    "        # commented because these graphs have their edges in lakhs\n",
    "        # (\"web-Google\", \"https://snap.stanford.edu/data/web-Google.txt.gz\"),\n",
    "        # (\"amazon0601\", \"https://snap.stanford.edu/data/amazon0601.txt.gz\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- Create directories and download ---\n",
    "for size, graphs in datasets.items():\n",
    "    os.makedirs(f\"raw_graphs/{size}\", exist_ok=True)\n",
    "\n",
    "    for name, url in graphs:\n",
    "        filename = os.path.basename(url)\n",
    "        path = f\"raw_graphs/{size}/{filename}\"\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            print(f\"‚ö° Skipping {name}: already downloaded ({path})\")\n",
    "            continue\n",
    "\n",
    "        print(f\"‚¨áÔ∏è Downloading {name} ...\")\n",
    "\n",
    "        try:\n",
    "            with requests.get(url, stream=True, verify=False, timeout=60) as r:\n",
    "                r.raise_for_status()  # raise error if download fails\n",
    "                with open(path, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "            print(f\"‚úÖ Saved: {path}\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Failed to download {name}: {e}\\n\")\n",
    "\n",
    "print(\"üéØ All requested graph datasets processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ae1e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting facebook_combined.txt.gz...\n",
      "‚úÖ Saved adjacency list: ../data/large_graphs/facebook_combined.txt.adj (4039 nodes, 88234 edges)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_graph_large(path):\n",
    "    \"\"\"\n",
    "    Load a large graph in SNAP format (.txt or .txt.gz).\n",
    "    SNAP format: each line represents an edge \"u v\"\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(path)[1]\n",
    "\n",
    "    # For .gz compressed files\n",
    "    if ext == \".gz\":\n",
    "        with gzip.open(path, 'rt') as f:\n",
    "            G = nx.read_edgelist(f, comments=\"#\", nodetype=int)\n",
    "    # For plain .txt files\n",
    "    elif ext == \".txt\":\n",
    "        G = nx.read_edgelist(path, comments=\"#\", nodetype=int)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format for large graphs: {ext}\")\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def save_as_adjlist(G, output_path):\n",
    "    \"\"\"\n",
    "    Save the graph in adjacency list format:\n",
    "    Each line: node: neighbor1 neighbor2 neighbor3 ...\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for node in sorted(G.nodes()):\n",
    "            neighbors = \" \".join(str(n) for n in sorted(G.neighbors(node)))\n",
    "            f.write(f\"{node}: {neighbors}\\n\")\n",
    "\n",
    "\n",
    "def convert_large_graphs(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Convert all SNAP-format graphs from input_folder to adjacency list format,\n",
    "    and save them in output_folder.\n",
    "    \"\"\"\n",
    "    # ‚úÖ Ensure output folder exists (relative to project root)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for fname in os.listdir(input_folder):\n",
    "        path = os.path.join(input_folder, fname)\n",
    "        if not os.path.isfile(path):\n",
    "            continue\n",
    "\n",
    "        print(f\"üîÑ Converting {fname}...\")\n",
    "\n",
    "        try:\n",
    "            # 1Ô∏è‚É£ Load graph\n",
    "            G = load_graph_large(path)\n",
    "\n",
    "            # 2Ô∏è‚É£ Generate output filename (replace .txt/.gz with .adj)\n",
    "            out_name = os.path.splitext(fname)[0] + \".adj\"\n",
    "            out_path = os.path.join(output_folder, out_name)\n",
    "\n",
    "            # 3Ô∏è‚É£ Save adjacency list\n",
    "            save_as_adjlist(G, out_path)\n",
    "\n",
    "            print(f\"‚úÖ Saved adjacency list: {out_path} ({len(G.nodes())} nodes, {len(G.edges())} edges)\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Skipping {fname}: {e}\\n\")\n",
    "\n",
    "\n",
    "# --- Run the conversion ---\n",
    "if __name__ == \"__main__\":\n",
    "    # ‚úÖ Ensure paths point to the project root (not /experiments)\n",
    "    convert_large_graphs(\"raw_graphs/large\", \"../data/large_graphs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23faae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bron kerbosch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
